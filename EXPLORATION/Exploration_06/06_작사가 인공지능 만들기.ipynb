{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06_작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#들어가며\" data-toc-modified-id=\"들어가며-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>들어가며</a></span></li><li><span><a href=\"#시퀀스?-스퀀스!\" data-toc-modified-id=\"시퀀스?-스퀀스!-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>시퀀스? 스퀀스!</a></span></li><li><span><a href=\"#I-다음-am을-쓰면-반-이상은-맞더라\" data-toc-modified-id=\"I-다음-am을-쓰면-반-이상은-맞더라-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>I 다음 am을 쓰면 반 이상은 맞더라</a></span><ul class=\"toc-item\"><li><span><a href=\"#언어-모델-(Language-Model)\" data-toc-modified-id=\"언어-모델-(Language-Model)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>언어 모델 (Language Model)</a></span></li></ul></li><li><span><a href=\"#실습-(1)-데이터-다듬기\" data-toc-modified-id=\"실습-(1)-데이터-다듬기-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>실습 (1) 데이터 다듬기</a></span></li><li><span><a href=\"#실습-(2)-인공지능-학습시키기\" data-toc-modified-id=\"실습-(2)-인공지능-학습시키기-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>실습 (2) 인공지능 학습시키기</a></span></li><li><span><a href=\"#실습-(3)-잘-만들어졌는지-평가하기\" data-toc-modified-id=\"실습-(3)-잘-만들어졌는지-평가하기-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>실습 (3) 잘 만들어졌는지 평가하기</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 들어가며\n",
    "\n",
    "'문장'이란 것은 무엇일까요? 막상 떠올리려니 참 어렵지 않나요? 사전적 정의는 아래와 같습니다.\n",
    "\n",
    "> 생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 단위.\n",
    "\n",
    "어떤 생각이나 감정을 말과 글로 표현하다… 멋지지만 인공지능에게 알려주기엔 조금 어려워 보입니다. 하지만 많은 인공지능들이 이미 놀라운 수준의 작문을 해내고 있다는 사실, 알고 계시나요? 처음 듣는 얘기라고요? 그렇다면 아래 웹사이트에서 현존하는 최고의 인공지능 작문가, GPT-2에게 작문을 시켜보세요! (현재는 GPT-3 모델도 발표되었습니다!)\n",
    "\n",
    "+ https://talktotransformer.com/\n",
    "\n",
    "어떤가요? 아직 한국어 작문 실력은 조금 어색한 듯도 하군요. 아직 한국어 데이터를 충분히 학습한 것 같지는 않아 보입니다. 그러나 GPT-2가 처음 나왔을 때엔 세간에 충격적인 인상을 남겼습니다. 관련하여 다음과 같은 기사를 한번쯤 보셨을지도 모르겠군요.\n",
    "\n",
    "GPT-2 이전에도 작문을 할 수 있는 딥러닝 모델은 존재했습니다. 그러나 생성한 문장 길이가 일정 이상이 되면 주제의 일관성이 흐트러지면서 어색함이 드러나곤 했습니다. 그러나 GPT-2는 무려 신문기사 1편 정도의 길이의 글을 작문하면서 주제나 논리의 일관성을 어느정도 유지했다는 점에서 놀라움을 주었습니다. 2019년 2월, GPT-2를 발표한 OpenAI에서 문장 생성 모델의 오남용이 가져올 위험 때문에 해당 모델을 비공개하기로 하면서 위와 같은 기사들이 한동안 세간의 이슈가 된 바 있습니다. 그로부터 1년 여 후 2020년 5월에 OpenAI에서는 다시 GPT-2를 이전보다 훨씬 큰 규모로 발전시킨 GPT-3를 발표해서 다시한번 충격을 주었습니다. GPT-3가 만들어낸 텍스트는 그저 논리적 일관성을 유지하는 수준을 넘어서서 사람이 쓴 것과 구분이 안될 정도의 자연스러움을 보여 주었기 때문입니다.\n",
    "\n",
    "왜 문장을 생성하는 인공지능이 이토록 충격을 주는 것일까요? 인공지능의 대명사와 같은 알파고 같은 모델도 있지만, 일반인들이 흔히 `인공지능`이라고 할 때 흔히 떠올리는 것은 바로 인간과 자연어로 대화 가능한 인공지능이기 때문입니다. 언어능력이야 말로 인간을 다른 동물과 구분해 주는 독특한 능력 아닐까요? 만약 인공지능이 사람의 언어에서 생각과 의도와 감정을 읽어 내고, 그 의미를 이해하며, 적절한 말을 만들어내서 인간의 질문에 대답할 수 있다면 그야말로 `지능을 가진 기계`라는 특이점에 이르렀다고 할 수 있기 때문입니다.\n",
    "\n",
    "이번 코스에서 우리는 **인공지능이 문장을 이해하는 방식**과 **작문을 가르치는 법**을 배울 겁니다. 코스를 성공적으로 마치고 나면 **멋진 인공지능 작사가**가 만들어져 있을 거고요! 부디 즐거운 시간이 되시길 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시퀀스? 스퀀스!\n",
    "\n",
    "**시퀀스(Sequence)** 라니? 시작부터 낯선 단어를 보여드려 죄송합니다. 하지만 우리는 이 시퀀스란 단어와 친해져야만 해요!\n",
    "\n",
    "생각보다 시퀀스는 널리 사용되고 있습니다. 영화 분야에서도, 전기 분야에서도 쓰이죠. 게다가 학창시절에 배운 수열을 영어로 시퀀스라고 한답니다!\n",
    "\n",
    "문장은 당연하고, 주가, 날짜, 심지어 드라마까지… 아주 많은 유형이 시퀀스 데이터에 포함됩니다. 그리고 그 데이터들을 \"Sequential\" 하다고 표현하죠. 도대체 기준이 뭘까요? 아주 멋지게 잘 설명한 글이 아래에 준비되어 있습니다. 5.2.1의 데이터 구조 부분을 읽어보세요.\n",
    "\n",
    "+ [파이썬 프로그래밍 입문서 (가제) - 5.2. 시퀀스](https://python.bakyeono.net/chapter-5-2.html)\n",
    "\n",
    "시퀀스 데이터가 곧 각 요소들의 연관성을 의미하는 것은 아니지만, 우리(인공지능)가 예측을 하려면 **어느 정도는 연관성이 있어줘야 합니다**. 예를 들어, `[ 18.01.01, 18.01.02, 18.01.03, ? ]` 의 \"?\" 부분을 맞추기 위해선 정답이 `18.01.04` 여야만 합니다. 정답이 \"오리\"라면 난감하다는 거죠!\n",
    "\n",
    "문장을 구성하는 각 단어들은 문법이라는 규칙을 따라 배열되어 있죠. 다만 이 `문법`이란 놈이 괘씸하다는 것을 우리는 오랜 교육과정을 통해 배웠습니다. 그런 의미에서 문장이라는 시퀀스 데이터는 꽤나 어렵습니다. 이런 문법을 인공지능이 그대로 배워서 문장 데이터를 예측할 리가 만무하니, 좀 더 단순한 접근 방법을 취해야겠죠. 바로 `통계에 기반한 방법`입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-20T19:26:47.585481Z",
     "start_time": "2021-01-20T19:26:47.580149Z"
    }
   },
   "source": [
    "## I 다음 am을 쓰면 반 이상은 맞더라\n",
    "\n",
    "> 통계라는 단어에서 거부감이 느껴지는 분들이 분명 계실 거라 생각해요. 적어도 이번 스텝에서는 쓸 수 있는 꿀팁을 전수해드리면 \"통계\" → \"대체로~\" 라고 바꿔 읽으시면 좋습니다.\n",
    "\n",
    "`나는 밥을 [ ]` 에서 빈 칸에 들어갈 말이 `먹는다`라는 것을 우리는 큰 고민 없이 알 수 있습니다. 밥은 통계적으로 먹히니까요. `알바생이 커피를 [ ]` 라면 아마도 `만든다`가 정답이겠죠. 알바생이 `커피`를 마실 수도 있지만, 통계적으론 만드니까요!\n",
    "\n",
    "인공지능이 글을 이해하게 하는 방식도 위와 같습니다. 어떤 문법적인 원리를 통해서가 아니고, **수많은 글을 읽게 함으로써** `나는` , `밥을`, 그 다음이 `먹는다` 라는 사실을 알게 하는 거죠. 그런 이유에서 **많은 데이터가 곧 좋은 결과**를 만들어냅니다. 단어를 적재적소에 활용하는 능력이 발달된다고 할까요?\n",
    "\n",
    "이 방식을 가장 잘 처리하는 인공지능 중 하나가 **순환신경망(RNN)** 입니다. 이번 시간엔 자세한 내용보다는 간단한 구조를 중심으로 공부하겠습니다.\n",
    "\n",
    "![img1](./img/img1.png)\n",
    "\n",
    "위는 순환신경망의 작동 방법을 가장 단순하게 표현한 예시입니다. 앞에서 `먹었다` 를 만드는 법은 배웠지만, 가장 첫 시작인 `나는` 은 어떻게 만들어야 할까요?\n",
    "\n",
    "이는 `<start>` 라는 특수한 토큰을 맨 앞에 추가해주므로써 해결할 수 있습니다. 인공지능에게 \"자, 이제 어떤 문장이든 생성해봐!\" 라는 사인을 주는 셈인거죠. `<start>` 를 입력으로 받은 순환신경망은 다음 단어로 `나는` 을 생성하고, **생성한 단어를 다시 입력으로 사용**합니다. 이 순환적인 특성을 살려 순환신경망이라고 이름을 붙인 것이죠!\n",
    "\n",
    "그렇게 순차적으로 `밥을 먹었다` 까지 생성하고나면, 인공지능은 \"다 만들었어!\" 라는 사인으로 `<end>` 라는 특수한 토큰을 생성합니다. 즉, 우리는 `<start>` 가 문장의 시작에 더해진 입력 데이터(문제지)와, `<end>` 가 문장의 끝에 더해진 출력 데이터(답안지)가 필요하며, 이는 **문장 데이터만 있으면 만들어낼 수 있다는** 것 또한 알 수 있습니다.\n",
    "\n",
    "위 과정을 **파이썬**으로는 아래와 같이 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T01:33:32.279785Z",
     "start_time": "2021-01-21T01:33:32.271917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 문장: <start> 나는 밥을 먹었다 \n",
      "Target 문장:  나는 밥을 먹었다 <end>\n"
     ]
    }
   ],
   "source": [
    "sentence = \" 나는 밥을 먹었다 \"\n",
    "\n",
    "source_sentence = \"<start>\" + sentence\n",
    "target_sentence = sentence + \"<end>\"\n",
    "\n",
    "print(\"Source 문장:\", source_sentence)\n",
    "print(\"Target 문장:\", target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언어 모델 (Language Model)\n",
    "\n",
    "`나는`, `밥을`, `먹었다` 를 순차적으로 생성할 때, `밥을` 다음이 `먹었다` 인 것은 쉽게 알 수 있습니다. 하지만 `나는` 다음이 `밥을` 인 것은 조금 억지처럼 느껴질 수 있습니다. 실제로 동작하는 방식도, `밥을` 을 만드는 것은 순전히 운입니다. 우리가 의도한다고 나오는 것이 아니죠.\n",
    "\n",
    "이걸 좀더 확률적으로 표현해 보겠습니다. '나는 밥을' 다음에 '먹었다' 가 나올 확률을 \n",
    "$ p(먹었다 | 나는, 밥을) $\n",
    " 이라고 합시다. 그렇다면 이 확률은 '나는' 뒤에 '밥이' 가 나올 확률인 \n",
    "$ p(밥을|나는) $\n",
    " 보다는 높게 나올 것입니다. 아마 \n",
    "$ p(먹었다 | 나는, 밥을, 맛있게) $\n",
    "의 확률값은 더 높아지겠죠?\n",
    "어떤 문구 뒤에 다음 단어가 나올 확률이 높다는 것은 그 다음 단어가 나오는 것이 보다 자연스럽다는 뜻이 됩니다. 그렇다면 '나는' 뒤에 '밥을'이 나오는 것이 자연스럽지 않다는 뜻일까요? 그것은 아닙니다. '나는' 뒤에 올 수 있는 자연스러운 단어의 경우의 수가 워낙 많다 보니 불확실성이 높을 뿐입니다.\n",
    "\n",
    "n-1개의 단어 시퀀스 \n",
    "$ w_1, \\cdots, w_{n-1} $ \n",
    "가 주어졌을 때, n번째 단어 \n",
    "$ w_n $\n",
    " 으로 무엇이 올지를 예측하는 확률 모델을 **언어 모델(Language Model)**이라고 부릅니다. 파라미터 \n",
    "$ θ $\n",
    "로 모델링하는 언어 모델을 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "$ P(w_n | w_1, …, w_{n-1};\\theta ) $\n",
    "\n",
    "잠깐 스크롤을 올려 RNN의 개념도를 잠깐 다시 보시면, 정확히 \n",
    "$ w_1, \\cdots, w_{n-1} $\n",
    "가 주어졌을 때, n번째 단어 \n",
    "$ w_n $\n",
    " 으로 무엇이 올지 예측하는 구조를 가지고 있음을 알아챌 수 있으실 겁니다. 이런 언어 모델을 어떻게 학습시킬 수 있을까요? 간단합니다. 어떤 텍스트도 언어 모델의 학습 데이터가 될 수 있습니다. n-1번째까지의 단어 시퀀스가 x_train이 되고 n번째 단어가 y_train이 되는 데이터셋은 무궁무진하게 만들 수 있으니까요. 이렇게 학습된 언어 모델을 학습 모드가 아닌 테스트 모드로 가동하면 어떤 일이 벌어질까요? 네, 이 모델은 일정한 단어 시퀀스가 주어진다면 다음 단어, 그 다음 단어를 계속해서 예측해 낼 것입니다. 이게 바로 텍스트 생성이고 작문 아니겠습니까? **잘 학습된 언어 모델은 훌륭한 문장 생성기**로 동작하게 됩니다.\n",
    "\n",
    "이전 스텝에서 소개했던 GPT-2 같은 문장 생성기도 언어 모델의 한 종류에 불과합니다. 딥러닝 모델의 구조나 파라미터 사이즈, 학습데이터의 양 등이 특별할 뿐, 기본적인 원리는 오늘 우리가 만들게 될 언어 모델과 전혀 다를게 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 (1) 데이터 다듬기\n",
    "\n",
    "지금부터는 실습을 해볼 겁니다. 이번 실습에서는 연극의 대사를 학습해서 스스로 연극 대사 문장을 생성해내는 언어 모델 인공지능을 만들 겁니다. 시작하죠!\n",
    "\n",
    "그럼 첫번째 단계는 무엇일까요? 인공지능의 시작은 언제나 그렇듯 데이터입니다.\n",
    "\n",
    "우선 다음과 같이 작업 디렉토리를 설정해 줍시다.\n",
    "\n",
    "그리고 이번 실습에서 사용할 데이터를 다운로드받아 작업 디렉토리로 옮겨 줍시다. 이번 실습에서는 텐서플로우(TensorFlow)가 제공하는 셰익스피어의 연극 대본을 사용할 겁니다.\n",
    "\n",
    "1Mb 남짓한 텍스트 파일이 받아졌습니다. GPT-2를 학습시킬 때의 100Gb 가까이 되는 수준과는 아주 거리가 멉니다만, 멋진 셰익스피어같은 극작가 언어 모델이 탄생하길 기대해 봅니다.\n",
    "\n",
    "우선 실습에 사용할 라이브러리를 불러와주세요. 그리고 방금 다운받은 파일의 내용을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T01:44:46.769600Z",
     "start_time": "2021-01-21T01:44:45.549029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import os\n",
    "\n",
    "# 파일을 읽기모드로 열어 봅니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()   # 텍스트를 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "print(raw_corpus[:9])    # 앞에서부터 10라인만 화면에 출력해 볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경로를 따라 데이터를 읽어오면 데이터가 어떻게 생겼는지 눈으로 확인할 수 있습니다. 완벽한 연극 대본이군요! 하지만 우린 **문장(대사)**만을 원하므로 화자 이름이나 공백뿐인 정보는 필요가 없습니다. 우리가 만들 언어 모델은 연극 대사를 만들어 내는 모델이거든요.\n",
    "\n",
    "1차 필터링을 할 필요가 있겠습니다. 데이터의 형태를 자세히 살피며 필터를 구상해보죠.\n",
    "\n",
    "![img2](./img/img2.png)\n",
    "\n",
    "모든 문장을 하나하나 검사한다고 가정합시다. 우리가 원치 않는 문장은 **화자가 표기된 문장(0, 3, 6)**, 그리고 **공백인 문장(2, 5, 9)** 입니다. 화자가 표기된 문장은 문장의 끝이 `:`로 끝나게 되어 있죠. 일반적으로 대사가 `:` 로 끝나는 일은 없을테니, `:` 를 기준으로 문장을 제외시켜도 괜찮을 것 같습니다. 그리고 공백인 문장은 길이를 검사하여 길이가 0이라면 제외를 시키죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T01:48:05.666341Z",
     "start_time": "2021-01-21T01:48:05.662342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 원하는 문장만 성공적으로 출력됩니다.\n",
    "\n",
    "텍스트 분류 모델에서 많이 보신 것처럼 텍스트 생성 모델에도 단어 사전을 만들게 됩니다. 그렇다면 문장을 일정한 기준으로 쪼개야겠죠? 그 과정을 **토큰화(Tokenize)** 라고 합니다.\n",
    "\n",
    "가장 심플한 방법은 띄어쓰기를 기준으로 나누는 방법이고, 우리도 그 방법을 사용할 겁니다. 하지만 약간의 문제가 있을 수 있죠. 몇 가지 문제 케이스를 살펴보죠.\n",
    "\n",
    "1. Hi, my name is John. \\*(\"Hi,\" \"my\", …, \"john.\" 으로 분리됨) - 문장부호\n",
    "\n",
    "2. First, open the first chapter. \\*(First와 first를 다른 단어로 인식) - 대소문자\n",
    "\n",
    "3. He is a ten-year-old boy. \\*(ten-year-old를 한 단어로 인식) - 특수문자\n",
    "\n",
    "\"1.\" 을 막기 위해 **문장 부호 양쪽에 공백을 추가** 할 거고요, \"2.\" 를 막기 위해 **모든 문자들을 소문자로 변환**할 겁니다. \"3.\"을 막기 위해 **특수문자들은 모두 제거**하도록 하죠!\n",
    "\n",
    "이런 전처리를 위해 정규표현식(Regex)을 이용한 필터링이 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T01:58:24.894464Z",
     "start_time": "2021-01-21T01:58:24.888597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "짜잔, 지저분한 문장을 넣어도 예쁘게 변환해주는 정제 함수가 완성되었습니다! 보너스로 이전 스텝에서 배운 `<start>` `<end>`도 추가했습니다.\n",
    "\n",
    "그러면 우리가 구축해야 할 데이터셋은 어떤 모양이 될까요?\n",
    "\n",
    "이전 스텝에서 봤던 예를 떠올려 봅시다.\n",
    "\n",
    "```\n",
    "언어 모델의 입력 문장 :  <start> 나는 밥을 먹었다\n",
    "언어 모델의 출력 문장 : 나는 밥을 먹었다 <end>\n",
    "```\n",
    "\n",
    "자연어처리 분야에서 모델의 입력이 되는 문장을 **소스 문장(Source Sentence)**, 정답 역할을 하게 될 모델의 출력 문장을 **타겟 문장(Target Sentence)**라고 관례적으로 부릅니다. 각각 X_train, y_train 에 해당한다고 할 수 있겠죠?\n",
    "\n",
    "그렇다면 우리는 위에서 만든 정제 함수를 통해 만든 데이터셋에서 토큰화를 진행한 후 끝 단어 `<end>`를 없애면 소스 문장, 첫 단어 `<start>`를 없애면 타겟 문장이 되겠죠? 이 정제 함수를 활용해서 아래와 같이 정제 데이터를 구축합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:01:43.477238Z",
     "start_time": "2021-01-21T02:01:43.213074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터는 완벽하게 준비가 된 것 같네요!\n",
    "\n",
    "자, 새로운 언어를 배우는 상상을 해봅시다. 영어를 전혀 모르던 그 때로 돌아가서, 다시 영어를 배우려면 어떻게 해야 할까요? **영한사전** 을 허리춤에 끼고 문장 속 단어를 하나하나 찾아가며 **한국어 해석** 을 적겠죠? 이 아이디어는 인공지능에게도 똑같이 적용됩니다. **배우고자 하는 언어** 를 **모국어로 표현** 을 해야 공부를 할 수 있어요.\n",
    "\n",
    "인공지능의 모국어라면 단연 **숫자**겠죠. 우리는 가르칠 언어(데이터)를 숫자로 변환해서 인공지능에게 줄 겁니다. 이에 필요한 것은 **사전**! 굳이 명명하자면… 데숫사전…?\n",
    "\n",
    "텐서플로우는 자연어 처리를 위한 여러 가지 모듈을 제공하는데, 우리도 그 모듈을 십분 활용할 겁니다! 아래에서 활용하게 될 `tf.keras.preprocessing.text.Tokenizer` 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 한 방에 해줍니다. 이 과정을 **벡터화(vectorize)** 라 하며, 숫자로 변환된 데이터를 **텐서(tensor)** 라고 칭합니다. 우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것입니다.\n",
    "\n",
    "> 텐서(tensor)는 굉장히 어려운 물리학 및 수학 개념입니다. 우리가 그 내용을 모두 이해할 필요는 없으나 아래의 웹페이지가 설명하는 간단한 개념정도는 알고 있으면 좋습니다.\n",
    ">\n",
    "> + [Tensor란 무엇인가?](https://rekt77.tistory.com/102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:27:00.944284Z",
     "start_time": "2021-01-21T02:27:00.426881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f3616ff1a50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:28:34.076256Z",
     "start_time": "2021-01-21T02:28:34.066947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24015, 21)\n",
      "[[   2  143   40  933  140  591    4  124   24  110]\n",
      " [   2  110    4  110    5    3    0    0    0    0]\n",
      " [   2   11   50   43 1201  316    9  201   74    9]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서 데이터는 모두 정수로 이루어져 있습니다. 이 숫자는 다름 아니라, tokenizer에 구축된 단어 사전의 인덱스입니다. 단어 사전이 어떻게 구축되었는지 아래와 같이 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:29:12.654807Z",
     "start_time": "2021-01-21T02:29:12.643727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떻습니까? 2번 인덱스가 바로 `<start>`였습니다. 왜 모든 행이 2로 시작하는지 이해할 수 있겠습니다.\n",
    "\n",
    "이제 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하겠습니다. 이 과정도텐서플로우가 제공하는 모듈을 사용할 것이니, 어떻게 사용하는지만 눈여겨 봐둡시다.\n",
    "\n",
    "텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워넣은 것입니다. 사전에는 없지만 0은 바로 패딩 문자 `<pad>`가 될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:31:30.232230Z",
     "start_time": "2021-01-21T02:31:30.227157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]   # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus 내의 첫번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았습니다. 예상대로 소스는 2(`<start>`)에서 시작해서 3(`<end>`)으로 끝난 후 0(`<pad>`)로 채워져 있습니다. 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한칸 시프트한 형태를 가지고 있습니다.\n",
    "\n",
    "마지막으로 우리는 데이터셋 객체를 생성할 것입니다. 그동안 우리는 model.fit(x_train, y_train, …) 형태로 Numpy Array 데이터셋을 생성하여 model에 제공하는 형태의 학습을 많이 진행해 왔습니다. 그러나 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 `tf.data.Dataset`객체를 생성하는 방법을 흔히 사용합니다. `tf.data.Dataset`객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의기능을 제공하므로 꼭 사용법을 알아 두시기를 권합니다. 우리는 이미 데이터셋을 텐서 형태로 생성해 두었으므로, `tf.data.Dataset.from_tensor_slices()` 메소드를 이용해 `tf.data.Dataset`객체를 생성할 것입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:40:03.957797Z",
     "start_time": "2021-01-21T02:40:03.798049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 스텝에서 데이터셋을 생성하기 위해 거쳐 온 과정을 잘 기억해 두시길 바랍니다.\n",
    "\n",
    "+ 정규표현식을 이용한 corpus 생성\n",
    "+ `tf.keras.preprocessing.text.Tokenizer`를 이용해 corpus를 텐서로 변환\n",
    "+ `tf.data.Dataset.from_tensor_slices()`를 이용해 corpus 텐서를 `tf.data.Dataset`객체로 변환\n",
    "\n",
    "`dataset`을 얻음으로써 데이터 다듬기 과정은 끝났습니다. tf.data.Dataset에서 제공하는 `shuffle()`, `batch()` 등 다양한 데이터셋 관련 기능을 손쉽게 이용할 수 있게 되었군요.\n",
    "\n",
    "이 모든 일련의 과정을 텐서플로우에서의 **데이터 전처리** 라 칭합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 (2) 인공지능 학습시키기\n",
    "\n",
    "> 우리가 인공지능이라고 부르는 것은 인공신경망이자 딥러닝 네트워크이자 이번 코스에선 순환신경망이기도 하고…… 너무 많은 이름이 같은 의미를 담고 있습니다. 따라서 지금부터 우리가 만들고자 하는 인공지능을 모델(model)이라고 칭하겠습니다. 실제로도 다들 모델이라고 한답니다.\n",
    "\n",
    "우리가 만들 모델의 구조도는 아래와 같습니다.\n",
    "\n",
    "![img3](./img/image3.png)\n",
    "\n",
    "우리가 만들 모델은 tf.keras.Model을 Subclassing하는 방식으로 만들 것입니다. 위 그림에서 설명한 것처럼 우리가 만들 모델에는 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있습니다.\n",
    "\n",
    "각 레이어의 기능을 확실히 이해하는 것은 나중에 하고, 지금은 구조도에 설명된 정도의 간단한 이해만 가지고 갑시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:51:01.883031Z",
     "start_time": "2021-01-21T02:51:01.857617Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 분류 모델을 다루어 보셨다면 Embedding 레이어의 역할에 대해서는 낯설지 않을 것입니다. 우리 입력 텐서에는 단어 사전의 인덱스가 들어 있습니다. Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 줍니다. 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용됩니다.\n",
    "\n",
    "위 코드에서 `embedding_size` 는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기입니다. 만약 그 크기가 2라면 예를 들어\n",
    "\n",
    "+ 차갑다: [0.0, 1.0]\n",
    "+ 뜨겁다: [1.0, 0.0]\n",
    "+ 미지근하다: [0.5, 0.5]\n",
    "\n",
    "정도의 구분이 가능하겠군요. 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 **오히려 혼란만을 야기**할 수 있습니다. 이번 실습에서는 256이 적당해 보이네요.\n",
    "\n",
    "LSTM 레이어의 hidden state 의 차원수인 `hidden_size` 도 같은 맥락입니다. `hidden_size` 는 모델에 얼마나 많은 일꾼을 둘 것인가? 로 이해해도 크게 엇나가지 않습니다. 그 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데, 역시 충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 **배가 산으로 갈 뿐** 입니다. 이번 실습에는 `1024`가 적당해보이는군요.\n",
    "\n",
    "---\n",
    "\n",
    "우리의 model은 아직 제대로 build되지 않았습니다. model.compile()을 호출한 적도 없고, 아직 model의 입력 텐서가 무엇인지 제대로 지정해 주지도 않았기 때문입니다.\n",
    "\n",
    "그런 경우 아래와 같이 model에 데이터를 아주 조금 태워 보는 것도 방법입니다. model의 input shape가 결정되면서 model.build()가 자동으로 호출됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:54:18.192316Z",
     "start_time": "2021-01-21T02:54:13.777898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-1.58890743e-05,  6.33748205e-05, -2.27023600e-04, ...,\n",
       "          1.69071180e-04, -1.13565453e-04,  1.89983242e-04],\n",
       "        [ 6.78111319e-05, -2.48930999e-04, -1.86194302e-04, ...,\n",
       "          4.31521534e-04, -6.72186143e-04,  6.71474299e-06],\n",
       "        [ 1.55273665e-04, -2.64225208e-04, -8.89550138e-05, ...,\n",
       "          2.51674588e-04, -9.98326926e-04, -9.24524356e-05],\n",
       "        ...,\n",
       "        [ 1.75745483e-03,  1.34401361e-03,  2.34312913e-03, ...,\n",
       "         -2.63075414e-03, -2.73525715e-04, -4.42852761e-04],\n",
       "        [ 2.21129251e-03,  1.67285604e-03,  2.51286849e-03, ...,\n",
       "         -2.75015598e-03, -2.61777779e-04, -5.90036099e-04],\n",
       "        [ 2.63154297e-03,  1.97349722e-03,  2.66523776e-03, ...,\n",
       "         -2.83544208e-03, -2.62950343e-04, -7.29362830e-04]],\n",
       "\n",
       "       [[-1.58890743e-05,  6.33748205e-05, -2.27023600e-04, ...,\n",
       "          1.69071180e-04, -1.13565453e-04,  1.89983242e-04],\n",
       "        [ 6.19406492e-05,  3.73086921e-04, -5.66293485e-04, ...,\n",
       "          2.02368872e-04, -2.31034806e-04,  1.17415337e-04],\n",
       "        [-4.93370928e-04,  4.60804382e-04, -8.13051302e-04, ...,\n",
       "          4.21378762e-04, -2.83009635e-04, -2.56531184e-05],\n",
       "        ...,\n",
       "        [ 2.95196986e-03,  2.50773970e-03,  1.98842259e-03, ...,\n",
       "         -2.21305201e-03, -2.93418969e-04, -1.59658655e-03],\n",
       "        [ 3.31465481e-03,  2.68335314e-03,  2.22031935e-03, ...,\n",
       "         -2.36280099e-03, -3.50118353e-04, -1.62256288e-03],\n",
       "        [ 3.62937758e-03,  2.84903590e-03,  2.42683711e-03, ...,\n",
       "         -2.47538160e-03, -3.91637412e-04, -1.64351950e-03]],\n",
       "\n",
       "       [[-1.58890743e-05,  6.33748205e-05, -2.27023600e-04, ...,\n",
       "          1.69071180e-04, -1.13565453e-04,  1.89983242e-04],\n",
       "        [-2.69550132e-04, -3.05318827e-04, -2.28041026e-04, ...,\n",
       "          3.60060949e-04,  1.27116673e-05,  2.73299898e-04],\n",
       "        [-6.89990236e-04, -5.85555681e-04, -6.31296833e-04, ...,\n",
       "          6.33155403e-04, -2.07299207e-04,  8.25539464e-05],\n",
       "        ...,\n",
       "        [ 3.02266702e-03,  1.61297410e-03,  1.19268405e-03, ...,\n",
       "         -2.81100348e-03, -3.31412040e-04, -7.49161118e-04],\n",
       "        [ 3.37079773e-03,  1.94672483e-03,  1.54650246e-03, ...,\n",
       "         -2.89318198e-03, -3.24131805e-04, -8.97236227e-04],\n",
       "        [ 3.67680448e-03,  2.24286248e-03,  1.86303724e-03, ...,\n",
       "         -2.94438889e-03, -3.20669817e-04, -1.02932076e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.58890743e-05,  6.33748205e-05, -2.27023600e-04, ...,\n",
       "          1.69071180e-04, -1.13565453e-04,  1.89983242e-04],\n",
       "        [-3.06565402e-04,  9.57001466e-05, -4.00400022e-04, ...,\n",
       "          4.20901168e-04, -2.71401776e-04,  2.56539584e-04],\n",
       "        [-1.98129143e-04,  2.14388318e-04, -5.69003168e-04, ...,\n",
       "          4.41286684e-04, -4.24577825e-04,  1.36097515e-04],\n",
       "        ...,\n",
       "        [ 3.20898066e-03,  2.62750173e-03,  1.78044010e-03, ...,\n",
       "         -2.80992943e-03,  1.17397656e-04, -1.17684877e-03],\n",
       "        [ 3.51271057e-03,  2.79781991e-03,  2.03164364e-03, ...,\n",
       "         -2.82916590e-03,  5.57205676e-05, -1.26104546e-03],\n",
       "        [ 3.78403161e-03,  2.95660482e-03,  2.25171680e-03, ...,\n",
       "         -2.83491961e-03, -1.44316300e-06, -1.33282179e-03]],\n",
       "\n",
       "       [[-1.58890743e-05,  6.33748205e-05, -2.27023600e-04, ...,\n",
       "          1.69071180e-04, -1.13565453e-04,  1.89983242e-04],\n",
       "        [-3.44284432e-04,  8.08149343e-05, -7.20609387e-04, ...,\n",
       "          3.06692935e-04, -1.08850785e-04,  1.42888777e-04],\n",
       "        [-1.05747728e-04,  9.57163211e-05, -9.42957180e-04, ...,\n",
       "          6.20666033e-05, -1.83360113e-04,  4.05110244e-04],\n",
       "        ...,\n",
       "        [ 1.71004597e-03,  6.96836098e-04,  1.30122120e-03, ...,\n",
       "         -3.58380238e-03,  7.98292283e-04, -7.52766791e-05],\n",
       "        [ 2.12306087e-03,  1.08490291e-03,  1.63429475e-03, ...,\n",
       "         -3.59404716e-03,  6.77536475e-04, -2.47728341e-04],\n",
       "        [ 2.52306997e-03,  1.44826772e-03,  1.93508004e-03, ...,\n",
       "         -3.57719441e-03,  5.52924816e-04, -4.09621513e-04]],\n",
       "\n",
       "       [[-1.58890743e-05,  6.33748205e-05, -2.27023600e-04, ...,\n",
       "          1.69071180e-04, -1.13565453e-04,  1.89983242e-04],\n",
       "        [-4.19205462e-04,  2.33657411e-04, -4.12458088e-04, ...,\n",
       "          5.05323347e-04, -3.48575691e-06,  3.97338154e-04],\n",
       "        [-1.24258094e-03,  3.04828340e-04, -6.01478911e-04, ...,\n",
       "          7.77275010e-04,  1.96479832e-05,  4.17224830e-04],\n",
       "        ...,\n",
       "        [ 2.25772127e-03,  2.22952920e-03,  1.84225908e-03, ...,\n",
       "         -2.84185563e-03, -1.30263716e-03,  3.19671119e-04],\n",
       "        [ 2.62724073e-03,  2.41947756e-03,  1.94315612e-03, ...,\n",
       "         -2.91780569e-03, -1.13082549e-03,  2.19602680e-05],\n",
       "        [ 2.97239539e-03,  2.59374548e-03,  2.06078123e-03, ...,\n",
       "         -2.96166725e-03, -9.86708561e-04, -2.49238044e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 최종 출력 텐서 shape를 유심히 보면 `shape=(256, 20, 7001)`임을 알 수 있습니다. 7001은 Dense 레이어의 출력 차원수입니다. 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문입니다.\n",
    "\n",
    "256은 이전 스텝에서 지정한 배치 사이즈입니다. `dataset.take(1)`를 통해서 1개의 배치, 즉 256개의 문장 데이터를 가져온 것입니다.\n",
    "\n",
    "그렇다면 20은 무엇을 의미할까요? 비밀은 바로 `tf.keras.layers.LSTM(hidden_size, return_sequences=True)`로 호출한 LSTM 레이어에서 `return_sequences=True`이라고 지정한 부분에 있습니다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 `return_sequences=False`였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다. 그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모른다는 점입니다. 모델을 만들면서 알려준 적도 없습니다. 그럼 20은 언제 알게된 것일까요? 네, 그렇습니다. 데이터를 입력받으면서 비로소 알게 된 것입니다. 우리 데이터셋의 max_len이 20으로 맞춰져 있었던 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:56:59.124097Z",
     "start_time": "2021-01-21T02:56:59.118406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 드디어 `model.summary()`를 호출할 수 있게 되었습니다. 그런데 호출해 보니 그동안 많이 보았던 것과는 다른 점이 있습니다. 우리가 궁금했던 Output Shape를 정확하게 알려주지 않습니다. 바로 위에서 설명한 이유 때문입니다. 우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없는 것입니다.\n",
    "\n",
    "하지만 모델의 파라미터 사이즈는 측정됩니다. 대략 22million 정도 되는군요. 참고로 서두에 소개했던 GPT-2의 파라미터 사이즈는, 1.5billion입니다. 우리 모델의 100배까지는 안되더라도 수십배가 넘는군요. 놀라지 마세요. GPT-3의 파라미터 사이즈는 GPT-2의 100배니까요.\n",
    "\n",
    "---\n",
    "\n",
    "이제 모델이 학습할 준비가 완료되었습니다. 아래 코드를 실행해 모델을 학습시켜보세요!\n",
    "\n",
    "> 학습엔 10분 정도 소요됩니다(GPU 환경 기준). 간단한 스트레칭과 커피 한 잔을 만들어 오기에 적당한 시간이죠.\n",
    "혹시라도 학습에 지나치게 많은 시간이 소요된다면 `tf.test.is_gpu_available()` 소스를 실행해 텐서플로우가 GPU를 잘 사용하고 있는지 확인하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T02:58:22.807990Z",
     "start_time": "2021-01-21T02:58:22.800655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T03:05:05.908441Z",
     "start_time": "2021-01-21T02:58:33.085847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 12s 133ms/step - loss: 3.5269\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 12s 134ms/step - loss: 2.8198\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 13s 135ms/step - loss: 2.7430\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.6545\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 13s 136ms/step - loss: 2.5827\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.5297\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.4798\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 2.4308\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 13s 137ms/step - loss: 2.3851\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 2.3414\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 2.3017\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 2.2622\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 2.2271\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 2.1925\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 13s 142ms/step - loss: 2.1579\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 14s 147ms/step - loss: 2.1220\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 13s 142ms/step - loss: 2.0892\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 13s 144ms/step - loss: 2.0536\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 2.0177\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 13s 138ms/step - loss: 1.9837\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.9467\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.9124\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.8784\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.8432\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.8094\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.7752\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.7415\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.7080\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.6742\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 13s 139ms/step - loss: 1.6418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3616ef9a10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss는 모델이 오답을 만들고 있는 정도라고 생각하셔도 좋습니다(그렇다고 Loss가 1일 때 99%를 맞추고 있다는 의미는 아닙니다). 오답률이 감소하고 있으니 **학습이 잘 진행되고 있다** 고 해석할 수 있죠!\n",
    "\n",
    "학습이 완료되었다면 이제 모델을 평가해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 (3) 잘 만들어졌는지 평가하기\n",
    "\n",
    "모델이 작문을 잘하는지 컴퓨터 알고리즘이 평가하는 것은 무리가 있습니다. 만약에 그게 가능했다면 우리가 지금껏 해온 독후감 숙제를 컴퓨터가 채점했겠죠? 따라서 작문 모델을 평가하는 가장 확실한 방법은 **작문을 시켜보고 직접 평가**하는 겁니다. 아래 `generate_text` 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T03:18:41.299364Z",
     "start_time": "2021-01-21T03:18:41.294672Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 생성하는 함수 안을 들여다보면 while문이 하나 자리잡고 있는 것을 볼 수 있습니다. 왜 그럴까요?\n",
    "\n",
    "학습 단계에서 우리는 이런 while 문이 필요없었습니다. 소스 문장과 타겟 문장이 있었고, 우리는 소스 문장을 모델에 입력해서 나온 결과를 타겟 문장과 직접 비교하면 그만이었습니다.\n",
    "그러나 텍스트를 실제로 생성해야 하는 시점에서, 우리에게는 2가지가 없습니다. 하나는 타겟 문장입니다. 또하나는 무엇이냐 하면, 소스 문장입니다. 생각해 보면 우리는 텍스트 생성 태스크를 위해 테스트 데이터셋을 따로 생성한 적이 없습니다.\n",
    "\n",
    "`generate_text()` 함수에서 `init_sentence`를 인자로 받고는 있습니다. 이렇게 받은 인자를 일단 텐서로 만들고 있습니다. 디폴트로는 `<start>` 단어 하나만 받는군요.\n",
    "\n",
    "+ while의 첫번째 루프에서 test_tensor에 `<start>` 하나만 들어갔다고 합시다. 우리의 모델이 출력으로 7001개의 단어 중 A를 골랐다고 합시다.\n",
    "+ while의 두번째 루프에서 test_tensor에는 `<start>` A가 들어갑니다. 그래서 우리의 모델이 그다음 B를 골랐다고 합시다.\n",
    "+ while의 세번째 루프에서 test_tensor에는 `<start>` A B가 들어갑니다. 그래서….. (이하 후략)\n",
    "\n",
    "---\n",
    "\n",
    "그럼 실제로 위 문장 생성 함수를 실행해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T03:19:54.127449Z",
     "start_time": "2021-01-21T03:19:54.048465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> tomorrow in the people , and the king of york <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> tomorrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제법 멋진 문장을 생성해냈군요! 위 함수의 `init_sentence` 를 바꿔가며 이런저런 실험을 해보세요! 단, `<start>`를 빼먹지는 않도록 합시다.\n",
    "\n",
    "> 지금은 짧은 한 줄짜리 문장을 생성하지만, 더 공부하시면 최고의 작문 모델 GPT-2를 이용해 더욱 멋진 문장 생성 프로젝트를 할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 참고자료\n",
    "\n",
    "+ https://app.inferkit.com/demo\n",
    "+ [뛰어난 '문장 생성 인공지능'을 숨겨야만 하는 이유는?](https://decenter.kr/NewsView/1VFGQMBBXZ/GZ02)\n",
    "+ [파이썬 프로그래밍 입문서 (가제) - 5.2. 시퀀스](https://python.bakyeono.net/chapter-5-2.html)\n",
    "+ [Tensor란 무엇인가?](https://rekt77.tistory.com/102)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
